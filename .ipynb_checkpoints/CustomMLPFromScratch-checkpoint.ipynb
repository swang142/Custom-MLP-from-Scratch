{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44243558-c951-4c65-a0e5-6b78feb90288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3723c-d3a9-4077-b47d-c3589aad701d",
   "metadata": {},
   "source": [
    "### **Common Activation Functions and their Derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7527881c-8c97-44b1-bdb6-3532cc9aba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z))\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "\n",
    "# Derivatives\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def tanh_derivative(Z):\n",
    "    return 1 - tanh(Z) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03635092-8d8b-414b-814e-ce6e99b4f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    'relu': relu,\n",
    "    'sigmoid': sigmoid,\n",
    "    'tanh': tanh,\n",
    "    'softmax': softmax,\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    'relu': relu_derivative,\n",
    "    'sigmoid': sigmoid_derivative,\n",
    "    'tanh': tanh_derivative,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef885e7a-40fd-4309-8999-8b20c70ae4bf",
   "metadata": {},
   "source": [
    "### **Common Loss Functions and their Derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e470b492-9caa-40a2-a5ab-725e3480dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def mse_loss(AL, Y):\n",
    "    return np.mean((AL - Y) ** 2)\n",
    "\n",
    "def binary_cross_entropy(AL, Y):\n",
    "    return -np.mean(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "\n",
    "def categorical_cross_entropy(AL, Y):\n",
    "    return -np.mean(np.sum(Y * np.log(AL), axis=0))\n",
    "\n",
    "# Derivatives \n",
    "def mse_loss_derivative(AL, Y):\n",
    "    return AL - Y\n",
    "\n",
    "def binary_cross_entropy_derivative(AL, Y):\n",
    "    return (AL - Y) / (AL * (1 - AL))\n",
    "\n",
    "def categorical_cross_entropy_derivative(AL, Y):\n",
    "    return AL - Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85888dc5-0532-4759-9595-cb3503c0d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = {\n",
    "    'mse': mse_loss,\n",
    "    'binary_cross_entropy': binary_cross_entropy,\n",
    "    'categorical_cross_entropy': categorical_cross_entropy\n",
    "}\n",
    "\n",
    "loss_derivatives = {\n",
    "    'mse': mse_loss_derivative,\n",
    "    'binary_cross_entropy': binary_cross_entropy_derivative,\n",
    "    'categorical_cross_entropy': categorical_cross_entropy_derivative\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6290054-627a-44c7-84b8-f2377243d0f6",
   "metadata": {},
   "source": [
    "### **Common Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c777cf10-2626-4ff2-95b9-19d5e498d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return np.mean(predictions == labels)\n",
    "\n",
    "def precision(predictions, labels):\n",
    "    true_positives = np.sum((predictions == 1) & (labels == 1))\n",
    "    predicted_positives = np.sum(predictions == 1)\n",
    "    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "    return precision\n",
    "\n",
    "def recall(predictions, labels):\n",
    "    true_positives = np.sum((predictions == 1) & (labels == 1))\n",
    "    actual_positives = np.sum(labels == 1)\n",
    "    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "08410d00-dc55-4396-9a30-0ebee244cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = {\n",
    "    'accuracy' : accuracy,\n",
    "    'precision' : precision,\n",
    "    'recall' : recall\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f87c2ac0-2d84-4aa2-a2ea-5f48c714c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLPNN:\n",
    "    def __init__(self, layer_sizes, activations, loss_function, learning_rate=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = loss_function\n",
    "        self.params = self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        params = {}\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            params['W' + str(i)] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1]) * 0.01\n",
    "            params['b' + str(i)] = np.zeros((self.layer_sizes[i], 1))\n",
    "        return params\n",
    "\n",
    "    def forward(self, X):\n",
    "        cache = {'A0': X}\n",
    "        A = X\n",
    "        L = len(self.layer_sizes) - 1\n",
    "        for i in range(1, L + 1):\n",
    "            Z = self.params['W' + str(i)].dot(A) + self.params['b' + str(i)]\n",
    "            A = activation_functions[self.activations[i-1]](Z)\n",
    "            cache['Z' + str(i)] = Z\n",
    "            cache['A' + str(i)] = A\n",
    "        return cache\n",
    "\n",
    "    def backprop(self, cache, Y, batch_size):\n",
    "        gradients = {}\n",
    "        L = len(self.layer_sizes) - 1\n",
    "        Y = Y.reshape(cache['A' + str(L)].shape)\n",
    "\n",
    "        # Output layer gradient\n",
    "        dZL = loss_derivatives[self.loss_function](cache['A' + str(L)], Y)\n",
    "        gradients['dW' + str(L)] = 1./batch_size * dZL.dot(cache['A' + str(L-1)].T)\n",
    "        gradients['db' + str(L)] = 1./batch_size * np.sum(dZL, axis=1, keepdims=True)\n",
    "\n",
    "        # Hidden layer gradients\n",
    "        dZ = dZL\n",
    "        for i in reversed(range(1, L)):\n",
    "            dA = self.params['W' + str(i+1)].T.dot(dZ)\n",
    "            dZ = dA * activation_derivatives[self.activations[i-1]](cache['Z' + str(i)])\n",
    "            gradients['dW' + str(i)] = 1./batch_size * dZ.dot(cache['A' + str(i-1)].T)\n",
    "            gradients['db' + str(i)] = 1./batch_size * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    # Basic SGD (Stochastic Gradient Descent)\n",
    "    def update_parameters(self, gradients):\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.params['W' + str(i)] -= self.learning_rate * gradients['dW' + str(i)]\n",
    "            self.params['b' + str(i)] -= self.learning_rate * gradients['db' + str(i)]\n",
    "            \n",
    "    def compute_cost(self, AL, Y):\n",
    "        return loss_functions[self.loss_function](AL, Y)\n",
    "\n",
    "    def make_predictions(self, X):\n",
    "        cache = self.forward(X)\n",
    "        predictions = cache['A' + str(len(self.layer_sizes) - 1)]\n",
    "        return predictions # returns the output layer\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs, batch_size):\n",
    "        m = X_train.shape[1]\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(m)\n",
    "            X_train_shuffled = X_train[:, permutation]\n",
    "            Y_train_shuffled = Y_train[:, permutation]\n",
    "\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X_train_shuffled[:, i:i+batch_size]\n",
    "                Y_batch = Y_train_shuffled[:, i:i+batch_size]\n",
    "\n",
    "                cache = self.forward(X_batch)\n",
    "                gradients = self.backprop(cache, Y_batch, batch_size)\n",
    "                self.update_parameters(gradients)\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                cache = self.forward(X_train)\n",
    "                cost = self.compute_cost(cache['A' + str(len(self.layer_sizes) - 1)], Y_train)\n",
    "                print(f\"Epoch {epoch}, Cost: {cost}\")\n",
    "                \n",
    "    def test(self, X_test, Y_test, metric='accuracy'):\n",
    "        predictions = self.make_predictions(X_test)   \n",
    "        if self.activations[-1] == 'softmax':\n",
    "            predictions = np.argmax(predictions, axis=0)\n",
    "            Y_test = np.argmax(Y_test, axis=0)\n",
    "        elif self.activations[-1] == 'sigmoid':\n",
    "            predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "        score = eval_metric[metric](predictions, Y_test)\n",
    "\n",
    "        print(f\"Test {metric.capitalize()}: {score:.4f}\")\n",
    "        return score\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
